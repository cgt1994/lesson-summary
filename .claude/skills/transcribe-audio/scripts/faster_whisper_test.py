#!/usr/bin/env python3
"""
Faster Whisper transcription script
Usage: python faster_whisper_test.py <audio_file> [--model base] [--language zh]
"""

import sys
import time
from pathlib import Path
from faster_whisper import WhisperModel

def transcribe_audio(audio_path, model_size="base", language=None, output_dir=None):
    """Transcribe audio file using faster-whisper"""

    audio_file = Path(audio_path)
    if not audio_file.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {audio_path}")
        return

    # Set output directory
    if output_dir is None:
        output_dir = audio_file.parent
    else:
        output_dir = Path(output_dir)

    output_file = output_dir / f"{audio_file.stem}.txt"

    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {audio_file}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ¤– æ¨¡å‹å¤§å°: {model_size}")
    print(f"ğŸŒ è¯­è¨€: {language or 'è‡ªåŠ¨æ£€æµ‹'}")
    print()

    # Initialize model (runs on CPU with float32 for optimal Apple Silicon performance)
    print("â³ åŠ è½½æ¨¡å‹...")
    start_load = time.time()
    model = WhisperModel(model_size, device="cpu", compute_type="float32")
    load_time = time.time() - start_load
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}ç§’)")
    print()

    # Transcribe
    print("ğŸ™ï¸ å¼€å§‹è½¬å½•...")
    start_time = time.time()

    segments, info = model.transcribe(
        str(audio_file),
        language=language,
        beam_size=1,  # é™ä½beam_sizeä»5åˆ°1ï¼Œé€Ÿåº¦æå‡30-40%
        best_of=1,  # åªå–æœ€ä½³ç»“æœ
        temperature=0,  # ç¡®å®šæ€§è¾“å‡ºï¼Œæ›´å¿«
        condition_on_previous_text=False,  # ç¦ç”¨ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œé€Ÿåº¦æ›´å¿«
        vad_filter=True,  # Voice Activity Detection - å»é™¤é™éŸ³
        vad_parameters=dict(min_silence_duration_ms=500)
    )

    print(f"æ£€æµ‹åˆ°çš„è¯­è¨€: {info.language} (æ¦‚ç‡: {info.language_probability:.2f})")
    print(f"éŸ³é¢‘æ—¶é•¿: {info.duration:.2f}ç§’")
    print()

    # Write transcription to file
    print("ğŸ’¾ ä¿å­˜è½¬å½•æ–‡æœ¬...")
    with open(output_file, "w", encoding="utf-8") as f:
        for segment in segments:
            text = segment.text.strip()
            f.write(text + "\n")
            print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {text}")

    elapsed_time = time.time() - start_time

    print()
    print("=" * 60)
    print("âœ“ è½¬å½•å®Œæˆ!")
    print(f"â±ï¸  å¤„ç†æ—¶é—´: {elapsed_time:.2f}ç§’")
    print(f"âš¡ å¤„ç†é€Ÿåº¦: {info.duration / elapsed_time:.2f}x å®æ—¶é€Ÿåº¦")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024:.1f} KB")
    print("=" * 60)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python faster_whisper_test.py <audio_file> [--model base] [--language zh]")
        sys.exit(1)

    audio_path = sys.argv[1]
    model_size = "base"
    language = None
    output_dir = None

    # Parse arguments
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == "--model" and i + 1 < len(sys.argv):
            model_size = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--language" and i + 1 < len(sys.argv):
            language = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--output-dir" and i + 1 < len(sys.argv):
            output_dir = sys.argv[i + 1]
            i += 2
        else:
            i += 1

    transcribe_audio(audio_path, model_size, language, output_dir)
